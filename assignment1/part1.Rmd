## Exercise 1

**a)** 
```{r include=FALSE}
Ice_cream.1 <- read.csv("./data/Ice_cream-1.csv")
mean_video <- mean(Ice_cream.1$video)
``` 

From the given dataset $icecream.csv$ the sample mean $m$ is `r mean_video`

```{r, echo=FALSE}
# Create a histogram
par(mfrow=c(1,3))
boxplot(Ice_cream.1$video, ylab="Value of Video", main="Boxplot of video")
hist(
  Ice_cream.1$video,
  xlim = c(20, 80),
  xlab="Value of video",
  ylim = c(0, 40),
  main="Histogram of video"
)
qqnorm(Ice_cream.1$video, pch = 1, frame = FALSE, ylim = c(20, 80), main="QQ-plot of video")
qqline(Ice_cream.1$video, col = "steelblue", lwd = 2)
``` 

When analyzing the histogram and QQ-plot above the assumption of a normal distribution can be made as the boxplot looks symmetrical, the histogram is bell shaped and the QQ-plot follows a relatively straight line.

```{r include=FALSE}
Ice_cream.1 <- read.csv("./data/Ice_cream-1.csv")
mean_video <- mean(Ice_cream.1$video)
sd_video <- sd(Ice_cream.1$video)
cl <- 0.97
z_score <- qnorm((1 + cl) / 2)
margin_of_error <- z_score * (sd_video / sqrt(length(Ice_cream.1$video)))

lower_bound <- mean_video - margin_of_error
upper_bound <- mean_video + margin_of_error
``` 
The 97% Confidence Interval the 95% bootstrap confidence interval for the population mean of video is [`r ceiling(lower_bound)`, `r ceiling(upper_bound)`] with the mean `r ceiling(mean_video)`

```{r include=FALSE}
sample_sd <- sd(Ice_cream.1$video)
required_sample_size <- (qnorm(0.985)^2 * sample_sd^2) / (3^2)
``` 
Calculating the minimal sample size $n$ with formula $n = \frac{(z_{\alpha/2})^2 \cdot \sigma^2}{E^2}$ for a length $E^3$ with the sample standard deviation of `r ceiling(sample_sd)` and a z-value which is found by $qnorm(0.985)$` as there is 0.015 in both tails the minimal sample size $n$ found is `r ceiling(required_sample_size)`
```{r include=TRUE}
B <- 1000
Tstar=numeric(B)
for(i in 1:B) {
  Xstar=sample(Ice_cream.1$video,replace=TRUE)
  Tstar[i]=mean(Xstar) 
}
T1<- mean(Tstar)
Tstar15 <- quantile(Tstar, 0.015)
Tstar985 <- quantile(Tstar, 0.985)
upper_bound <- 2*T1-Tstar985
lower_bound <- 2*T1-Tstar15
``` 

When bootstrapping on the video column using the above code generating 1000 values the 95% bootstrap confidence interval for the population mean of video is [`r ceiling(upper_bound)`, `r ceiling(lower_bound)`] with the mean `r ceiling(mean_video)`. When comparing these values to the earlier done CI we see the values are exactly the same.
**b)** 
```{r include=FALSE}
# Assuming your data is in a variable called 'scores'
# H0: The mean score is equal to 50
# H1: The mean score is greater than 50

# One-sample t-test
result <- t.test(Ice_cream.1$video, alternative = "greater", mu = 50)

# Print the result
print(result)

# Access confidence interval
conf_interval <- result$conf.int
p_value <- result$p.value
print(conf_interval)
``` 
Looking at 1 a from the plots we can see a normal distribution which justifies the usage of a t-test. When doing a one sample t-test with $H_0$: $\mu$ is equal to 50 and $H_a$: $\mu$ is greater than 50; the following result for the p-value is found: `r p_value`. When taking a significance level of 0.05 we can thus reject the null hypothesis in favor of the alternative hypothesis $\mu$ is greater than 50. When looking at the Confidence Interval (CI) of the same alpha we can assume the lower bound, mean and upper bound are above 50 since the null hypothesis is rejected.

**c)** 
```{r include=FALSE}
# Number of observations greater than 6
success_count <- sum(Ice_cream.1$video > 50)

# Total number of observations
total_count <- length(Ice_cream.1$video)

# Binomial test
binom_test_result <- binom.test(success_count, total_count, p = 0.5, alternative = "greater")
video <- sum(rank(abs(Ice_cream.1$video-50))[Ice_cream.1$video-50>0])

wilcox_test_result <- wilcox.test(video, mu=50, alternative="greater")

# Define the fraction and the expected proportion
observed_fraction <- sum(Ice_cream.1$video < 42) / length(Ice_cream.1$video)
expected_proportion <- 0.25  # the expected proportion under the null hypothesis

# Perform a one-sample proportion test
prop_test_result <- prop.test(sum(Ice_cream.1$video < 42), length(Ice_cream.1$video), p = expected_proportion, alternative = "less")

# Display the test result
prop_test_result
``` 

When using the sign test in R first of all the sum of column $video$ which is bigger then 50 have to be found which is `r success_count`. The total length of the column is `r total_count`. Now doing the Binominal test with these values and the null hypothesis: The median value is 50, and the alternative hypothesis the median is greater then 50 gives a p-value of `r binom_test_result$p.value`. When taking the significance level 0.05 there can thus be concluded the null hypothesis cannot be rejected and thus that there is not enough evidence to conclude that the median value is greater than 50.

When using the Wilcox signed rank test on the video column with $\mu$ = 50 and hypothesizes H0: The median value is equal to 50 and Ha: The median value is bigger then 50 the p-value found is `r wilcox_test_result$p.value`. When using the significance level 0.05 we can therefore there is not enough evidence to reject the null hypothesis that the median value is equal to 50.

When comparing these tests to the tests done in _b)_ the conclusion can be made that both the median and the mean are not above 50.

When using a one sample proportion test with H0: The proportion is equal to 25% or bigger and Ha: The proportion is smaller then 25% the value `r prop_test_result$p.value` is found. When using a significance level of 0.05 the null hypothesis can thus be rejected which means that the fraction of the scores less than 42 is at most 25%.


**d)** 

To do the described bootstrap test, one could do the following:

```{r echo=TRUE}
data <- Ice_cream.1$video
n <- length(Ice_cream.1)
t = min(Ice_cream.1)

B <- 1000
candidate_mus <- 1:100 
p_values = numeric(100)
for (mu in candidate_mus) {
  Tstar = numeric(B)
  for (i in 1:B) {
      xstar <- rnorm(n, mean=mu, sd=100)
      Tstar[i] <- min(xstar)
  }
  p_left <- sum(Tstar < t)/B
  p_right <- sum(Tstar > t)/B
  p_values[mu] <- 2 * min(p_left, p_right)
}
``` 

Performing this test is not useful, however, as the question asks to sample from $N(\mu, 100)$, which has a standard deviation that is way higher than the estimated one from the sample, and which ultimately means that none of the means from 1 to 100 can be rejected. This is not to mention that doing a bootstrap test with a presumed normal sample does not make sense either way, as this is a distribution that can be tested for directly, no bootstrapping needed.

Performing a Kolmogrov-Smirnov test here could theoretically be done, but does not make a ton of sense either. One _could_ take a single sample output from `rnorm` and then use this test to see if it comes from the same distribution as the sample data, and do this for all $mu$ candidates to get the list asked for, but this would simply be a less powerful version of the already weird thing that is done with the bootstrap test, as we are using a test that tests two sample population distributions by first throwing away the information of the exact distribution we are testing against by sampling from it, and again throwing away the fact that we are presuming a analyzable distribution by using a non-parametric test.

**e)** 

```{r echo=FALSE}
male_scores <- Ice_cream.1[Ice_cream.1$female == 1, "video"]
female_scores <- Ice_cream.1[Ice_cream.1$female == 0, "video"]

# Two-Sample t-test
t_test_result <- t.test(male_scores, female_scores)

# Mann-Whitney test
mannwhitney_result <- wilcox.test(male_scores, female_scores)

# Kolmogorov-Smirnov test
ks_test_result <- ks.test(male_scores, female_scores)

par(mfrow=c(1,2))
qqnorm(male_scores, pch = 1, frame = FALSE, ylim = c(20, 80), main="QQ-plot of males")
qqline(male_scores, col = "steelblue", lwd = 2)

qqnorm(female_scores , pch = 1, frame = FALSE, ylim = c(20, 80), main="QQ-plot of females")
qqline(female_scores , col = "steelblue", lwd = 2)
``` 

First, the male and female values have to be extracted into a variable using the $female$ column in the provided CSV. Secondly, an analysis of the QQ_plots should be done, to check for normality. Inspecting these, they do not appear to by quite normal.

_t-test_

Null Hypothesis (H0): The mean scores of male and female students are equal.
Alternative Hypothesis (Ha): The mean score of male students is different from female students.

The resulting p-value is `r t_test_result$p.value` when using a significant level of 0.05 this thus means the null hypothesis is not rejected.

_Man-Whitney test_

Null Hypothesis (H0): There is no difference in the distribution of scores between male and female students.
Alternative Hypothesis (H1): The distribution of scores is different between male and female students

The resulting p-value is `r mannwhitney_result$p.value` when using a significant level of 0.05 this thus means the null hypothesis is not rejected.

_Kolmogorov-Smirnov test_

Null Hypothesis (H0): The distributions of scores for male and female students are the same.
Alternative Hypothesis (H1): The distributions of scores for male and female students are different.

The resulting p-value is `r ks_test_result$p.value` when using a significant level of 0.05 this thus means the null hypothesis is not rejected.

Of these tests, doing Man-Whitney or Kolmogorov-Smirnov makes the most sense, as we concluded from the qq-plots that normality cannot be presumed. However, none of them end up rejecting the null hypothesis; all of them come to the same conclusion.

A permutation test would not be applicable, as the dataset is not paired.

**f)** 
```{r echo=FALSE}
par(mfrow=c(1,2))
qqnorm(Ice_cream.1$video, pch = 1, frame = FALSE, ylim = c(20, 80), main="QQ-plot of video")
qqline(Ice_cream.1$video, col = "steelblue", lwd = 2)

qqnorm(Ice_cream.1$puzzle, pch = 1, frame = FALSE, ylim = c(20, 80), main="QQ-plot of puzzle")
qqline(Ice_cream.1$puzzle, col = "steelblue", lwd = 2)

correlation_result <- cor.test(Ice_cream.1$video, Ice_cream.1$puzzle, method="spearman")
``` 

When looking at the QQ-plot for puzzle it seems doubtful that it is normally distributed. Due to this reason Spearman's correlation test between the columns video and puzzle is used. The hypothesizes are as followed: $H_0$: There is no correlation between the two columns and $H_a$: There is correlation between the columns. the results of the Pearson’s correlation test gives a P-value of `r correlation_result$p.value`. When using a significance level of 0.05 the conclusion can be drawn that the null hypothesis can be rejected and there is thus enough evidence to conclude that the columns puzzle and video are significantly correlated, if normality is assumed.

```{r echo=FALSE}
wilcoxon_test_result <- wilcox.test(Ice_cream.1$puzzle, Ice_cream.1$video, paired=TRUE, alternative = "greater")
``` 

To verify if the distribution of these values is significantly different a Wilcoxon test was done. Using the following hypothesizes:

Null Hypothesis: The distribution of scores in the column puzzle is equal to the distribution of scores in the column video.

Alternative Hypothesis: The distribution of scores in the column puzzle is greater than the distribution of scores in the column video.

The resulting p-value is `r wilcoxon_test_result$p.value`. Using a significance level of 0.05 we can thus conclude that we fail to rejecto the null hypothesis and there is generally speaking no difference in scores.
